# üìä Data Warehouse & ETL Pipeline: AdventureWorks

> **Projeto Acad√™mico de Engenharia de Dados** | UniSales
> **Autor:** Mikael

---

## üìë Resumo do Projeto
Este projeto consiste na implementa√ß√£o de um pipeline de dados completo (**End-to-End**) para a constru√ß√£o de um Data Warehouse (OLAP). O objetivo foi extrair dados transacionais do ERP **AdventureWorks** (Microsoft SQL Server), transform√°-los utilizando **Python/Pandas** e carreg√°-los em um modelo dimensional (Star Schema) no **PostgreSQL**, utilizando o **Apache Airflow** como orquestrador.

O diferencial deste projeto foi a utiliza√ß√£o de uma arquitetura 100% baseada em cont√™ineres **Docker**, simulando um ambiente de produ√ß√£o isolado e reprodut√≠vel.

---

## üèóÔ∏è Arquitetura da Solu√ß√£o

A infraestrutura foi definida como c√≥digo (IaC) atrav√©s do `docker-compose`, integrando tr√™s servi√ßos principais:

| Servi√ßo | Tecnologia | Fun√ß√£o |
| :--- | :--- | :--- |
| **Origem (OLTP)** | Microsoft SQL Server 2019 (Linux) | Banco transacional contendo os dados brutos de vendas e compras (`AdventureWorks2019`). |
| **Orquestrador** | Apache Airflow 2.10 | Respons√°vel pelo agendamento, execu√ß√£o e monitoramento das DAGs de ETL. |
| **Destino (OLAP)** | PostgreSQL 13 | Data Warehouse modelado em *Star Schema* para an√°lises de BI. |
| **Processamento** | Python 3.12 + Pandas | Engine de transforma√ß√£o e limpeza de dados executada nos *Workers* do Airflow. |

---

## üíé Modelagem Dimensional (Star Schema)

O foco anal√≠tico deste projeto foi o setor de **Compras (Purchasing)**, visando analisar a efici√™ncia de fornecedores e custos de aquisi√ß√£o.

### üîπ Tabela Fato
* **`Fato_Compras`**: Granularidade por item de pedido de compra.
    * M√©tricas: `Qtd_Pedida`, `Qtd_Recebida`, `Qtd_Rejeitada` (Qualidade), `Valor_Unitario`, `Frete_Rateado`.

### üî∏ Tabelas Dimens√£o
* **`Dim_Produto`**: Dados descritivos dos produtos, categorias e subcategorias.
* **`Dim_Fornecedor`**: Informa√ß√µes sobre os parceiros comerciais, localiza√ß√£o e classifica√ß√£o de cr√©dito.
* **`Dim_Tempo`**: Calend√°rio can√¥nico para an√°lises temporais (Ano, M√™s, Trimestre, Dia da Semana).

---

## ‚öôÔ∏è O Processo ETL (Extract, Transform, Load)

O pipeline foi codificado na DAG `etl_compras_adventureworks`, seguindo as etapas:

1.  **Extraction (Extra√ß√£o):** Conex√£o ao SQL Server via driver ODBC (`mssql-tools18`) utilizando `MsSqlHook`. Leitura dos dados brutos das tabelas `Purchasing.PurchaseOrderHeader` e `Detail`.
2.  **Transformation (Transforma√ß√£o):**
    * Limpeza de dados e tratamento de nulos com Pandas.
    * Gera√ß√£o de Chaves Substitutas (Surrogate Keys) para a dimens√£o Tempo.
    * C√°lculo de m√©tricas derivadas (ex: Custo Total da Linha).
3.  **Loading (Carga):**
    * Inser√ß√£o dos dados nas tabelas do PostgreSQL.
    * Uso de estrat√©gia *Truncate-Insert* para garantir a consist√™ncia dos dados em ambiente de desenvolvimento.

---

## üöß Desafios T√©cnicos e Troubleshooting

Durante o desenvolvimento, o projeto superou desafios significativos relacionados √† infraestrutura local:

### 1. OOM Killed (Out of Memory)
A execu√ß√£o simult√¢nea do SQL Server, PostgreSQL e da stack completa do Airflow (Webserver, Scheduler, Triggerer) excedeu os recursos de hardware dispon√≠veis no ambiente WSL2.
* **Impacto:** O container do SQL Server entrava em estado de reinicializa√ß√£o constante (Exit Code 137).
* **Contorno:** Foi necess√°rio implementar estrat√©gias de reinicializa√ß√£o controlada e otimiza√ß√£o da aloca√ß√£o de mem√≥ria no `.wslconfig`.

### 2. Drivers e Conectividade
A comunica√ß√£o entre o Airflow (baseado em Debian Linux) e o SQL Server exigiu a customiza√ß√£o da imagem Docker.
* **Solu√ß√£o:** Cria√ß√£o de um `Dockerfile` personalizado para instala√ß√£o das bibliotecas de sistema `unixodbc-dev` e o driver propriet√°rio `msodbcsql17` da Microsoft.

### 3. Instabilidade da Interface Gr√°fica
Devido √† carga no sistema, a UI do Airflow apresentou lat√™ncia.
* **Solu√ß√£o:** Gerenciamento das DAGs (Unpause/Trigger) e cria√ß√£o de conex√µes realizados via **Airflow CLI** diretamente no terminal do container, garantindo a execu√ß√£o mesmo sem acesso √† interface web.

---

## üöÄ Como Executar o Projeto

Requisitos: Docker e Docker Desktop instalados.

1.  **Clonar o reposit√≥rio:**
    ```bash
    git clone [https://github.com/SEU-USUARIO/etl-adventureworks-airflow.git](https://github.com/SEU-USUARIO/etl-adventureworks-airflow.git)
    ```

2.  **Adicionar o Dataset:**
    Baixe o arquivo `AdventureWorks2019.bak` e coloque na raiz do projeto (o arquivo √© ignorado pelo Git devido ao tamanho).

3.  **Subir o Ambiente:**
    ```bash
    docker compose up --build -d
    ```

4.  **Restaurar o Banco de Origem:**
    Executar o script de restore via `docker exec` para popular o SQL Server containerizado.

5.  **Acessar o Airflow:**
    Navegar para `http://localhost:8080` (Login: `airflow` / `airflow`) e ativar a DAG.

---

**Nota:** Este projeto foi desenvolvido com foco acad√™mico para demonstrar a profici√™ncia em ferramentas de Engenharia de Dados.
